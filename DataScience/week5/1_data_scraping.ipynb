{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and examining HTML page\n",
    "\n",
    "Import \n",
    "* `requests` for loading web pages\n",
    "* `math` for math operations\n",
    "* `bs4` for loading *BeautifulSoup* for working with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the BeautifulSoup page and print part of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n",
      "\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n",
      "<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"/nb/themes/Default/nb.css\">\n",
      "<meta name=\"Description\" content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML\n",
      "9321\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.crummy.com/software/BeautifulSoup'\n",
    "source = requests.get(url).text\n",
    "print(source[:500])\n",
    "print(len(source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is word `Alice` in the source of the web page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('Alice' in source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count occurences of word `Soup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(source.count('Soup'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find index of 'foo.com' substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3528\n"
     ]
    }
   ],
   "source": [
    "foo = 'foo.com'\n",
    "position =  source.find(foo)\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test to see the substring in the source variable. You can access strings like lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo.com\n"
     ]
    }
   ],
   "source": [
    "print(source[position:position + 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or you can use a tidier version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo.com\n"
     ]
    }
   ],
   "source": [
    "print(source[position:position + len(foo)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting `BeautifulSoup` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output raw HTML.\n",
    "`print(soup)` would print the whole string representation of `BeautifulSoup` object, use `print(soup.__str__()[:1000])` to first transform `BeautifulSoupbs4` into its string representation and then pring first 1000 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n",
      "\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n",
      "<link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n",
      "<link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n",
      "<meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n",
      "<meta content=\"Leonard Richardson\" name=\"author\"/>\n",
      "</head>\n",
      "<body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n",
      "<img align=\"right\" src=\"10.1.jpg\" width=\"250\"/><br/>\n",
      "<p>You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.</p>\n",
      "<div align=\"center\">\n",
      "<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\n",
      "<p>\"A tremendous boon.\" -- Python411 Podcast</p>\n",
      "<p>[ <a href=\"#Download\">Download</a> | <a href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/NEWS.txt\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\n",
      "<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "<p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p></small></p>\n",
      "</div>\n",
      "<p><i>If you have questions, send them to <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\n",
      "group</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>\n",
      "<p>Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "\n",
      "<ol>\n",
      "<li>Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "\n",
      "<li>Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "\n",
      "<li>Beautiful Soup sits on top of popular Python parsers like <a href=\"http://lxml.de/\">lxml</a> and <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "\n",
      "</li></li></li></ol>\n",
      "<p>Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "\n",
      "<p>Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "\n",
      "<p>Interested? <a href=\"bs4/doc/\">Read more.</a>\n",
      "<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\n",
      "<p>The current release is <a href=\"bs4/download/\">Beautiful Soup\n",
      "4.6.3</a> (August 12, 2018). You can install Beautiful Soup 4 with\n",
      "<code>pip install beautifulsoup4</code>.\n",
      "\n",
      "<p>In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "<code>python-bs4</code> package (for Python 2) or the\n",
      "<code>python3-bs4</code> package (for Python 3). In Fedora it's\n",
      "available as the <code>python-beautifulsoup4</code> package.\n",
      "\n",
      "<p>Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the <code>bs4/</code> directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using <code>2to3</code>.)\n",
      "\n",
      "<p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\n",
      "\n",
      "<h3>Beautiful Soup 3</h3>\n",
      "<p>Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It is considered stable, and only\n",
      "critical security bugs will be fixed. <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here's\n",
      "the Beautiful Soup 3 documentation.</a>\n",
      "<p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\n",
      "the same license as Python itself.\n",
      "\n",
      "<p>The current release of Beautiful Soup 3 is <a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\n",
      "2012). You can install Beautiful Soup 3 with <code>pip install\n",
      "BeautifulSoup</code>. It's also available as\n",
      "<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\n",
      "<code>python-BeautifulSoup</code> in Fedora.\n",
      "\n",
      "<p>You can also download the tarball and use\n",
      "<code>BeautifulSoup.py</code> in your project directly.\n",
      "\n",
      "\n",
      "<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\n",
      "<p>Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "\n",
      "<ul>\n",
      "<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n",
      " Type\"</a>, a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "\n",
      "<li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\n",
      "a page that's been linked to and find a representative image</a>.\n",
      "\n",
      "<li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n",
      " activities</a> of an arms merchant.\n",
      "\n",
      "<li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\n",
      "bug tracker from Sourceforge to Roundup</a>.\n",
      "\n",
      "<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\n",
      "uses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\n",
      "statewide election results</a>.\n",
      "\n",
      "<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\n",
      "Applications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "</li></li></li></li></li></li></ul>\n",
      "<p>If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or <a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\n",
      "group</a>.\n",
      "\n",
      "<h2>Development</h2>\n",
      "<p>Development happens at <a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\n",
      "code</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\n",
      "bugs</a>.<hr/><table><tr><td valign=\"top\">\n",
      "<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Tuesday, October 23 2018, 16:25:41 Nowhere Standard Time and last built on Thursday, November 15 2018, 20:00:01 Nowhere Standard Time.</p><p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2018 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table></p></td></tr></table></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></body></html><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--><td valign=\"top\"><p><b>Document tree:</b>\n",
      "<dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\n",
      "</dd></dl>\n",
      "</dd></dl>\n",
      "\n",
      "\n",
      "Site Search:\n",
      "\n",
      "<form action=\"/search/\" method=\"get\">\n",
      "<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n",
      "</form>\n",
      "</p></td>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(soup.__str__()[:1000]) - prints string repo\n",
    "print(soup) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print formatted HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n",
      "\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <title>\n",
      "   Beautiful Soup: We called him Tortoise because he taught us.\n",
      "  </title>\n",
      "  <link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n",
      "  <link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n",
      "  <meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n",
      "  <meta content=\"Leonard Richardson\" name=\"author\"/>\n",
      " </head>\n",
      " <body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n",
      "  <img align=\"right\" src=\"10.1.jpg\" width=\"250\"/>\n",
      "  <br/>\n",
      "  <p>\n",
      "   You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "  </p>\n",
      "  <div align=\"center\">\n",
      "   <a href=\"bs4/download/\">\n",
      "    <h1>\n",
      "     Beautiful Soup\n",
      "    </h1>\n",
      "   </a>\n",
      "   <p>\n",
      "    \"A tremendous boon.\" -- Python411 Podcast\n",
      "   </p>\n",
      "   <p>\n",
      "    [\n",
      "    <a href=\"#Download\">\n",
      "     Download\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"bs4/doc/\">\n",
      "     Documentation\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"#HallOfFame\">\n",
      "     Hall of Fame\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"https://code.launchpad.net/beautifulsoup\">\n",
      "     Source\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/NEWS.txt\">\n",
      "     Changelog\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "     Discussion group\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"zine/\">\n",
      "     Zine\n",
      "    </a>\n",
      "    ]\n",
      "   </p>\n",
      "   <p>\n",
      "    <small>\n",
      "     If you use Beautiful Soup as part of your work, please consider a\n",
      "     <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">\n",
      "      Tidelift subscription\n",
      "     </a>\n",
      "     . This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "     <p>\n",
      "      If Beautiful Soup is useful to you on a personal level, you might like to read\n",
      "      <a href=\"zine/\">\n",
      "       <i>\n",
      "        Tool Safety\n",
      "       </i>\n",
      "      </a>\n",
      "      , a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "     </p>\n",
      "    </small>\n",
      "   </p>\n",
      "  </div>\n",
      "  <p>\n",
      "   <i>\n",
      "    If you have questions, send them to\n",
      "    <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "     the discussion\n",
      "group\n",
      "    </a>\n",
      "    . If you find a bug,\n",
      "    <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "     file it\n",
      "    </a>\n",
      "    .\n",
      "   </i>\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "   <ol>\n",
      "    <li>\n",
      "     Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "     <li>\n",
      "      Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "      <li>\n",
      "       Beautiful Soup sits on top of popular Python parsers like\n",
      "       <a href=\"http://lxml.de/\">\n",
      "        lxml\n",
      "       </a>\n",
      "       and\n",
      "       <a href=\"http://code.google.com/p/html5lib/\">\n",
      "        html5lib\n",
      "       </a>\n",
      "       , allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "      </li>\n",
      "     </li>\n",
      "    </li>\n",
      "   </ol>\n",
      "   <p>\n",
      "    Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class\n",
      "    <tt>\n",
      "     externalLink\n",
      "    </tt>\n",
      "    \", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "    <p>\n",
      "     Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "     <p>\n",
      "      Interested?\n",
      "      <a href=\"bs4/doc/\">\n",
      "       Read more.\n",
      "      </a>\n",
      "      <a name=\"Download\">\n",
      "       <h2>\n",
      "        Download Beautiful Soup\n",
      "       </h2>\n",
      "      </a>\n",
      "      <p>\n",
      "       The current release is\n",
      "       <a href=\"bs4/download/\">\n",
      "        Beautiful Soup\n",
      "4.6.3\n",
      "       </a>\n",
      "       (August 12, 2018). You can install Beautiful Soup 4 with\n",
      "       <code>\n",
      "        pip install beautifulsoup4\n",
      "       </code>\n",
      "       .\n",
      "       <p>\n",
      "        In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "        <code>\n",
      "         python-bs4\n",
      "        </code>\n",
      "        package (for Python 2) or the\n",
      "        <code>\n",
      "         python3-bs4\n",
      "        </code>\n",
      "        package (for Python 3). In Fedora it's\n",
      "available as the\n",
      "        <code>\n",
      "         python-beautifulsoup4\n",
      "        </code>\n",
      "        package.\n",
      "        <p>\n",
      "         Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the\n",
      "         <code>\n",
      "          bs4/\n",
      "         </code>\n",
      "         directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using\n",
      "         <code>\n",
      "          2to3\n",
      "         </code>\n",
      "         .)\n",
      "         <p>\n",
      "          Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\n",
      "          <h3>\n",
      "           Beautiful Soup 3\n",
      "          </h3>\n",
      "          <p>\n",
      "           Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It is considered stable, and only\n",
      "critical security bugs will be fixed.\n",
      "           <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">\n",
      "            Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "           </a>\n",
      "           <p>\n",
      "            Beautiful Soup 3 works only under Python 2.x. It is licensed under\n",
      "the same license as Python itself.\n",
      "            <p>\n",
      "             The current release of Beautiful Soup 3 is\n",
      "             <a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">\n",
      "              3.2.1\n",
      "             </a>\n",
      "             (February 16,\n",
      "2012). You can install Beautiful Soup 3 with\n",
      "             <code>\n",
      "              pip install\n",
      "BeautifulSoup\n",
      "             </code>\n",
      "             . It's also available as\n",
      "             <code>\n",
      "              python-beautifulsoup\n",
      "             </code>\n",
      "             in Debian and Ubuntu, and as\n",
      "             <code>\n",
      "              python-BeautifulSoup\n",
      "             </code>\n",
      "             in Fedora.\n",
      "             <p>\n",
      "              You can also download the tarball and use\n",
      "              <code>\n",
      "               BeautifulSoup.py\n",
      "              </code>\n",
      "              in your project directly.\n",
      "              <a name=\"HallOfFame\">\n",
      "               <h2>\n",
      "                Hall of Fame\n",
      "               </h2>\n",
      "              </a>\n",
      "              <p>\n",
      "               Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "               <ul>\n",
      "                <li>\n",
      "                 <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\n",
      "                  \"Movable\n",
      " Type\"\n",
      "                 </a>\n",
      "                 , a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "                 <li>\n",
      "                  Reddit uses Beautiful Soup to\n",
      "                  <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">\n",
      "                   parse\n",
      "a page that's been linked to and find a representative image\n",
      "                  </a>\n",
      "                  .\n",
      "                  <li>\n",
      "                   Alexander Harrowell uses Beautiful Soup to\n",
      "                   <a href=\"http://www.harrowell.org.uk/viktormap.html\">\n",
      "                    track the business\n",
      " activities\n",
      "                   </a>\n",
      "                   of an arms merchant.\n",
      "                   <li>\n",
      "                    The developers of Python itself used Beautiful Soup to\n",
      "                    <a href=\"http://svn.python.org/view/tracker/importer/\">\n",
      "                     migrate the Python\n",
      "bug tracker from Sourceforge to Roundup\n",
      "                    </a>\n",
      "                    .\n",
      "                    <li>\n",
      "                     The\n",
      "                     <a href=\"http://www2.ljworld.com/\">\n",
      "                      Lawrence Journal-World\n",
      "                     </a>\n",
      "                     uses Beautiful Soup to\n",
      "                     <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">\n",
      "                      gather\n",
      "statewide election results\n",
      "                     </a>\n",
      "                     .\n",
      "                     <li>\n",
      "                      The\n",
      "                      <a href=\"http://esrl.noaa.gov/gsd/fab/\">\n",
      "                       NOAA's Forecast\n",
      "Applications Branch\n",
      "                      </a>\n",
      "                      uses Beautiful Soup in\n",
      "                      <a href=\"http://laps.noaa.gov/topograbber/\">\n",
      "                       TopoGrabber\n",
      "                      </a>\n",
      "                      , a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "                     </li>\n",
      "                    </li>\n",
      "                   </li>\n",
      "                  </li>\n",
      "                 </li>\n",
      "                </li>\n",
      "               </ul>\n",
      "               <p>\n",
      "                If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or\n",
      "                <a href=\"http://groups.google.com/group/beautifulsoup/\">\n",
      "                 the discussion\n",
      "group\n",
      "                </a>\n",
      "                .\n",
      "                <h2>\n",
      "                 Development\n",
      "                </h2>\n",
      "                <p>\n",
      "                 Development happens at\n",
      "                 <a href=\"https://launchpad.net/beautifulsoup\">\n",
      "                  Launchpad\n",
      "                 </a>\n",
      "                 . You can\n",
      "                 <a href=\"https://code.launchpad.net/beautifulsoup/\">\n",
      "                  get the source\n",
      "code\n",
      "                 </a>\n",
      "                 or\n",
      "                 <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "                  file\n",
      "bugs\n",
      "                 </a>\n",
      "                 .\n",
      "                 <hr/>\n",
      "                 <table>\n",
      "                  <tr>\n",
      "                   <td valign=\"top\">\n",
      "                    <p>\n",
      "                     This document (\n",
      "                     <a href=\"/source/software/BeautifulSoup/index.bhtml\">\n",
      "                      source\n",
      "                     </a>\n",
      "                     ) is part of Crummy, the webspace of\n",
      "                     <a href=\"/self/\">\n",
      "                      Leonard Richardson\n",
      "                     </a>\n",
      "                     (\n",
      "                     <a href=\"/self/contact.html\">\n",
      "                      contact information\n",
      "                     </a>\n",
      "                     ). It was last modified on Tuesday, October 23 2018, 16:25:41 Nowhere Standard Time and last built on Thursday, November 15 2018, 20:00:01 Nowhere Standard Time.\n",
      "                    </p>\n",
      "                    <p>\n",
      "                     <table class=\"licenseText\">\n",
      "                      <tr>\n",
      "                       <td>\n",
      "                        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "                         <img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/>\n",
      "                        </a>\n",
      "                       </td>\n",
      "                       <td valign=\"top\">\n",
      "                        Crummy is © 1996-2018 Leonard Richardson. Unless otherwise noted, all text licensed under a\n",
      "                        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "                         Creative Commons License\n",
      "                        </a>\n",
      "                        .\n",
      "                       </td>\n",
      "                      </tr>\n",
      "                     </table>\n",
      "                    </p>\n",
      "                   </td>\n",
      "                  </tr>\n",
      "                 </table>\n",
      "                </p>\n",
      "               </p>\n",
      "              </p>\n",
      "             </p>\n",
      "            </p>\n",
      "           </p>\n",
      "          </p>\n",
      "         </p>\n",
      "        </p>\n",
      "       </p>\n",
      "      </p>\n",
      "     </p>\n",
      "    </p>\n",
      "   </p>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "<!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>-->\n",
      "<td valign=\"top\">\n",
      " <p>\n",
      "  <b>\n",
      "   Document tree:\n",
      "  </b>\n",
      "  <dl>\n",
      "   <dd>\n",
      "    <a href=\"http://www.crummy.com/\">\n",
      "     http://www.crummy.com/\n",
      "    </a>\n",
      "    <dl>\n",
      "     <dd>\n",
      "      <a href=\"http://www.crummy.com/software/\">\n",
      "       software/\n",
      "      </a>\n",
      "      <dl>\n",
      "       <dd>\n",
      "        <a href=\"http://www.crummy.com/software/BeautifulSoup/\">\n",
      "         BeautifulSoup/\n",
      "        </a>\n",
      "       </dd>\n",
      "      </dl>\n",
      "     </dd>\n",
      "    </dl>\n",
      "   </dd>\n",
      "  </dl>\n",
      "  Site Search:\n",
      "  <form action=\"/search/\" method=\"get\">\n",
      "   <input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n",
      "  </form>\n",
      " </p>\n",
      "</td>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `a` HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>,\n",
       " <a href=\"#Download\">Download</a>,\n",
       " <a href=\"bs4/doc/\">Documentation</a>,\n",
       " <a href=\"#HallOfFame\">Hall of Fame</a>,\n",
       " <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a>,\n",
       " <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/NEWS.txt\">Changelog</a>,\n",
       " <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>,\n",
       " <a href=\"zine/\">Zine</a>,\n",
       " <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>,\n",
       " <a href=\"zine/\"><i>Tool Safety</i></a>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('a')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('Soup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the first occurrence of `a` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_tag = soup.find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the value of `href` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bs4/download/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tag.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all HTML links from the web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = soup.findAll('a')\n",
    "links = []\n",
    "for tag in tags:\n",
    "   link = tag.get('href')\n",
    "   links.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively a shorter notation can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bs4/download/', '#Download', 'bs4/doc/', '#HallOfFame', 'https://code.launchpad.net/beautifulsoup']\n"
     ]
    }
   ],
   "source": [
    "link_list = [l.get('href') for l in soup.findAll('a')]\n",
    "print(link_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all external links by going in a loop through all the links and adding links starting with `http://` or `https://` to the resulting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7323dedaebfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexternal_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'http://'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https://'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m        \u001b[0mexternal_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "external_links = []\n",
    "for l in link_list:\n",
    "   if l[:7] == 'http://' or l[:8] == 'https://':\n",
    "       external_links.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-22-7323dedaebfc>\u001b[0m(3)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0mexternal_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'http://'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https://'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m       \u001b[0mexternal_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?\n",
    "Let's make sure that none of the strings we are checking are actually strings and not `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://code.launchpad.net/beautifulsoup', 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/NEWS.txt', 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup', 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website', 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup']\n"
     ]
    }
   ],
   "source": [
    "for l in link_list:\n",
    "    if l is not None and len(l) >= 7 and \\\n",
    "    (l[:7] == 'http://' or\n",
    "     l[:8] == 'https://'):\n",
    "        external_links.append(l)\n",
    "print(external_links[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing DOM tree with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a string with HTML code into a `bs4BeautifulSoup` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bs4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d4205b578f2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhtmlString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"<!DOCTYPE html><html><head>\"\u001b[0m     \u001b[1;34m\"<title>This is a title</title></head>\"\u001b[0m     \u001b[1;34m\"<body><h3> Test </h3><p>Hello world!</p</body></html>\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtmlString\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# get BeautifulSoup object\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bs4' is not defined"
     ]
    }
   ],
   "source": [
    "htmlString = \"<!DOCTYPE html><html><head>\" \\\n",
    "    \"<title>This is a title</title></head>\" \\\n",
    "    \"<body><h3> Test </h3><p>Hello world!</p</body></html>\"\n",
    "tree = bs4.BeautifulSoup(htmlString, \"html.parser\")  # get BeautifulSoup object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the `html` _root node_ of the DOM tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>This is a title</title></head><body><h3> Test </h3><p>Hello world!</p></body></html>\n"
     ]
    }
   ],
   "source": [
    "root_node = tree.html\n",
    "print(root_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `head` of the root using `contents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head><title>This is a title</title></head>\n"
     ]
    }
   ],
   "source": [
    "head = root_node.contents[0]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `body` from `root`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body><h3> Test </h3><p>Hello world!</p></body>\n"
     ]
    }
   ],
   "source": [
    "body = root_node.contents[1]\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, body can be accessed directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body><h3> Test </h3><p>Hello world!</p></body>\n"
     ]
    }
   ],
   "source": [
    "body = tree.body\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping case\n",
    "Scrape jobs portal www.indeed.com for information on data science skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a page from www.indeed.com with posts about Data Scientist job offers and load it into a `BeautifulSoup` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-38c4d2cba609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://www.indeed.com/jobs?q=data+scientist&l='\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbs_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'http://www.indeed.com/jobs?q=data+scientist&l='\n",
    "source = requests.get(url).text\n",
    "bs_tree = bs4.BeautifulSoup(source, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many job postings we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search yielded 32,003 hits.\n"
     ]
    }
   ],
   "source": [
    "job_count_string = bs_tree.find(id = 'searchCount').contents[0]\n",
    "job_count_string = job_count_string.split()[-2]\n",
    "print(\"Search yielded %s hits.\" % (job_count_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `job_count` so far is still a string, not an integer, and the `,` separator prevents us from just casting it to int. \n",
    "To convert string `\"32,006\"` into an integer we \n",
    "* iterate through all characters of the string, filtering out everything that is not a digit (`filter()` function),\n",
    "* concatenate elements of the resulting list with digit characters using `str.join()`,\n",
    "* convert the resulting string into an integer (`int()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '2', '0', '0', '6']\n"
     ]
    }
   ],
   "source": [
    "job_count_string = '32,006'\n",
    "job_count_digits = list(filter(lambda c: c in '1234567890', job_count_string))\n",
    "print(job_count_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "job_count = int(''.join(job_count_digits))\n",
    "print(type(job_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website is only listing 10 results per page,  so we need to scrape them page after page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.indeed.com/jobs?q=data+scientist'\n",
    "html_page = requests.get(url).text\n",
    "bs_tree = bs4.BeautifulSoup(html_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages left:  3\n",
      "We found a lot of jobs:  50\n"
     ]
    }
   ],
   "source": [
    "# num_pages = int(math.ceil(job_count/10.0))\n",
    "num_pages = 3\n",
    "\n",
    "base_url = 'http://www.indeed.com'\n",
    "job_links = []\n",
    "for i in range(5):  # do range(num_pages) if you want them all\n",
    "    if i%10==0:\n",
    "        print(\"Number of pages left: \", num_pages-i)\n",
    "    url = 'http://www.indeed.com/jobs?q=data+scientist&start=' + str(i*10)\n",
    "    html_page = requests.get(url).text\n",
    "    bs_tree = bs4.BeautifulSoup(html_page, 'html.parser')\n",
    "    \n",
    "    job_postings = []\n",
    "    job_posting_candidates = bs_tree.findAll(\"h2\", {\"class\": \"jobtitle\"})\n",
    "    for candidate in job_posting_candidates: \n",
    "        job_postings.append(candidate.findChildren()[0].get(\"href\"))\n",
    "    \n",
    "    for job in job_postings:     # go after each link\n",
    "        job_links.append(base_url + job)\n",
    "        \n",
    "print(\"We found a lot of jobs: \", len(job_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job postings left:  0 {'hadoop': 7, 'spark': 5}"
     ]
    }
   ],
   "source": [
    "skill_set = {'hadoop': 0, 'spark': 0}\n",
    "counter = 0\n",
    "\n",
    "for link in job_links:\n",
    "    counter +=1  \n",
    "    \n",
    "    try:\n",
    "        html_page = requests.get(link).text\n",
    "    except Exception:\n",
    "        continue  # proceed with next link in case of an error\n",
    "\n",
    "    html_text = html_page.lower()    \n",
    "    for key in skill_set.keys():\n",
    "        if key in html_text:  \n",
    "            skill_set[key] +=1\n",
    "            # skip the rest of the text as the term can occur several times\n",
    "            continue\n",
    "            \n",
    "    if counter % 5 == 0:\n",
    "        print(\"\\rJob postings left:\", len(job_links) - counter, skill_set,\n",
    "              end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Twitter API\n",
    "**Task**: List popular tweets of Donald Trump (with more than 1000000 retweets)\n",
    "\n",
    "Authenticate your app against Twitter with \n",
    "* consumer key\n",
    "* consumer secret\n",
    "* access token key\n",
    "* access token secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "\n",
    "cKey = 'tOZWzonsap454dtjS1aHy17Bd'\n",
    "cSecret = 'fRVCa1WpS8lapp9pVQdr82X4UamEsfMLDoobm0kkN9yepg8chz'\n",
    "aKey = '560363551-rDPjiQQ91OW4x8YE5OS8QySkmwus9SM8IBuTNF6J'\n",
    "aSecret = 'bU1cAoGTFIeoRbZkz23eeazy98rWZ5THzM43DsB3cBvms'\n",
    "\n",
    "## create the api object with the twitter-python library\n",
    "api = twitter.Api(consumer_key=cKey, consumer_secret=cSecret, \n",
    "                  access_token_key=aKey, access_token_secret=aSecret, tweet_mode=\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get timeline for user with screen_name 'realDonaldTrump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_statuses = api.GetUserTimeline(\n",
    "        screen_name = 'realDonaldTrump')\n",
    "statuses = [t.AsDict() for t in twitter_statuses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tweets with `retweet_count` larger than 1000000, which we will be considering as popular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Thu Nov 15 20:43:42 +0000 2018',\n",
       " 'favorite_count': 634,\n",
       " 'full_text': 'It is our sacred duty to support America’s Service Members every single day they wear the uniform – and every day after when they return home as Veterans. Together we will HONOR those who defend us, we will CHERISH those who protect us, and we will celebrate the amazing heroes... https://t.co/kovcIj4fwU',\n",
       " 'hashtags': [],\n",
       " 'id': 1063170699500642304,\n",
       " 'id_str': '1063170699500642304',\n",
       " 'lang': 'en',\n",
       " 'media': [{'display_url': 'pic.twitter.com/kovcIj4fwU',\n",
       "   'expanded_url': 'https://twitter.com/realDonaldTrump/status/1063170699500642304/video/1',\n",
       "   'id': 1063170305084944384,\n",
       "   'media_url': 'http://pbs.twimg.com/ext_tw_video_thumb/1063170305084944384/pu/img/F1wBFXepohxi2fOz.jpg',\n",
       "   'media_url_https': 'https://pbs.twimg.com/ext_tw_video_thumb/1063170305084944384/pu/img/F1wBFXepohxi2fOz.jpg',\n",
       "   'sizes': {'large': {'h': 720, 'resize': 'fit', 'w': 1280},\n",
       "    'medium': {'h': 675, 'resize': 'fit', 'w': 1200},\n",
       "    'small': {'h': 383, 'resize': 'fit', 'w': 680},\n",
       "    'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
       "   'type': 'video',\n",
       "   'url': 'https://t.co/kovcIj4fwU',\n",
       "   'video_info': {'aspect_ratio': [16, 9],\n",
       "    'duration_millis': 77183,\n",
       "    'variants': [{'bitrate': 256000,\n",
       "      'content_type': 'video/mp4',\n",
       "      'url': 'https://video.twimg.com/ext_tw_video/1063170305084944384/pu/vid/320x180/jthPvl7SSMgylQjw.mp4?tag=6'},\n",
       "     {'bitrate': 2176000,\n",
       "      'content_type': 'video/mp4',\n",
       "      'url': 'https://video.twimg.com/ext_tw_video/1063170305084944384/pu/vid/1280x720/toUASoODqLob1kJr.mp4?tag=6'},\n",
       "     {'content_type': 'application/x-mpegURL',\n",
       "      'url': 'https://video.twimg.com/ext_tw_video/1063170305084944384/pu/pl/25i3Tm-5eGYyLJf8.m3u8?tag=6'},\n",
       "     {'bitrate': 832000,\n",
       "      'content_type': 'video/mp4',\n",
       "      'url': 'https://video.twimg.com/ext_tw_video/1063170305084944384/pu/vid/640x360/faAIVzCtynZ1SWWw.mp4?tag=6'}]}}],\n",
       " 'retweet_count': 190,\n",
       " 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       " 'urls': [],\n",
       " 'user': {'created_at': 'Wed Mar 18 13:46:38 +0000 2009',\n",
       "  'description': '45th President of the United States of America🇺🇸',\n",
       "  'favourites_count': 7,\n",
       "  'followers_count': 55704731,\n",
       "  'following': True,\n",
       "  'friends_count': 46,\n",
       "  'geo_enabled': True,\n",
       "  'id': 25073877,\n",
       "  'id_str': '25073877',\n",
       "  'lang': 'en',\n",
       "  'listed_count': 96138,\n",
       "  'location': 'Washington, DC',\n",
       "  'name': 'Donald J. Trump',\n",
       "  'profile_background_color': '6D5C18',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/25073877/1540264974',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/874276197357596672/kUuht00m_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/874276197357596672/kUuht00m_normal.jpg',\n",
       "  'profile_link_color': '1B95E0',\n",
       "  'profile_sidebar_border_color': 'BDDCAD',\n",
       "  'profile_sidebar_fill_color': 'C5CEC0',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'screen_name': 'realDonaldTrump',\n",
       "  'statuses_count': 39690,\n",
       "  'url': 'https://t.co/OMxB0xp8tD',\n",
       "  'verified': True},\n",
       " 'user_mentions': []}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statuses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maybe_interesting = []\n",
    "for status in statuses:\n",
    "    if status['retweet_count'] > 10000:\n",
    "        maybe_interesting.append(status)\n",
    "\n",
    "# same using `filter`        \n",
    "# maybe_interesting = \\\n",
    "#    filter(lambda status: \n",
    "#        status['retweet_count']>50000, \n",
    "#        statuses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of everything except texts of those tweets and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######\n",
      "The only “Collusion” is that of the Democrats with Russia and many others. Why didn’t the FBI take the Server from the DNC? They still don’t have it. Check out how biased Facebook, Google and Twitter are in favor of the Democrats. That’s the real Collusion!\n",
      "######\n",
      "Universities will someday study what highly conflicted (and NOT Senate approved) Bob Mueller and his gang of Democrat thugs have done to destroy people. Why is he protecting Crooked Hillary, Comey, McCabe, Lisa Page &amp; her lover, Peter S, and all of his friends on the other side?\n",
      "######\n",
      "....care how many lives the ruin. These are Angry People, including the highly conflicted Bob Mueller, who worked for Obama for 8 years. They won’t even look at all of the bad acts and crimes on the other side. A TOTAL WITCH HUNT LIKE NO OTHER IN AMERICAN HISTORY!\n",
      "######\n",
      "The inner workings of the Mueller investigation are a total mess. They have found no collusion and have gone absolutely nuts. They are screaming and shouting at people, horribly threatening them to come up with the answers they want. They are a disgrace to our Nation and don’t...\n",
      "######\n",
      "The White House is running very smoothly and the results for our Nation are obviously very good. We are the envy of the world. But anytime I even think about making changes, the FAKE NEWS MEDIA goes crazy, always seeking to make us look as bad as possible! Very dishonest!\n"
     ]
    }
   ],
   "source": [
    "tweet_texts = list(map(lambda status: \n",
    "                       status['full_text'], \n",
    "                       maybe_interesting))\n",
    "for t in tweet_texts[:5]:\n",
    "    print('######')\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
